+++
# Date this page was created.
date = 2016-04-27T00:00:00

# Project title.
title = "1. Neural processing of speech"

# Project summary to display on homepage.
summary = ""

# Optional image to display on homepage (relative to `static/img/` folder).
image_preview = "brain.jpg"

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ["EEG", "L2"]

# Optional external URL for project (replaces project detail page).
external_link = ""

# Does the project detail page use math formatting?
math = false

# Optional featured image (relative to `static/img/` folder).
#[header]
#image = "headers/bubbles-wide.jpg"
#caption = "My caption :smile:"

+++
Listening to a second language (L2) in real life situations (e.g., a noisy restaurant) can be challenging. Native listeners can flexibly modulate their speech processing to overcome the demands of a noisy environment e.g., use semantic-contextual information to understand what is being said. We know very little, however, about the strategies second-language listeners use to enhance their listening. Using electroencephalography (EEG), my PhD work published in Cognition demonstrated that greater listening effort exerted by L2 listeners enhances their auditory processing, as measured by neural tracking of speech, even though their linguistic processes (e.g., lexical-semantic processing as measured by N400) were less developed than those of native listeners.

This work subsequently led to a 3-year ESRC research grant on listening effort and multilingual speech communication where I worked as a postdoctoral researcher (PI: Prof. Paul Iverson). You can see a summary of this project here, and please find related work on Publications.

* People feel like they need to "listen harder" when communicating in a second language, but it isn't clear how this effort changes the brain processes involved in recognising speech. Our initial research has produced a surprising finding; we tested people who were trying to listen to a talker in a noisy background (i.e., a distracting talker), and found that auditory areas of the brain are better at picking out the target talker when people are listening to a second language than their first language. We did this by recording neural activity (electroencephalography; EEG) and measuring how it becomes entrained to the acoustics of speech. Although people would normally be expected to perform better when listening to their first language, we think that second-language listeners had more selective auditory processing because of their additional listening effort. We found related effects for neural measures of word recognition in the same task, and think that we've found mechanisms that allow second-language learners to partially compensate for their speech recognition difficulties. In this grant project, we will expand our investigation in a series of studies that manipulate the acoustics of speech, and compare how speech is recognised in first and second languages by speakers of English and Korean. Furthermore, we will test adults who learned both languages at the same time when they were young children, adults who learned their second language later in life, and older children who are in the process of learning both languages. Our goals are to understand how people can use listening effort to compensate for their difficulties with second-language speech, and examine how this ability develops and relates to proficiency. This work is important for understanding how people apply their processes and structures for language during everyday speech communication, and is relevant to a wide range of difficult listening conditions (e.g., hearing impairment). The work will also advance our scientific understanding of how new measures of neural entrainment for speech relate to practical aspects of speech recognition.