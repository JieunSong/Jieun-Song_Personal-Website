<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jieun Song on Jieun Song</title>
    <link>/</link>
    <description>Recent content in Jieun Song on Jieun Song</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Evaluating Korean learners’ English rhythm proficiency with measures of sentence stress</title>
      <link>/publication/appliedpsycholing/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/publication/appliedpsycholing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Native and non-native speech recognition in noise: neural measures of auditory and lexical processing</title>
      <link>/publication/icphs2019/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/publication/icphs2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>언어의 아이들: 아이들은 도대체 어떻게 언어를 배울까?</title>
      <link>/publication/book/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0100</pubDate>
      
      <guid>/publication/book/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interaction of semantic and prosodic cues in the disambiguation of wh-words in Korean</title>
      <link>/talk/oxford/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/oxford/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coping with noise at multiple levels: Auditory cortical and lexical effects of maskers for native and non-native listeners</title>
      <link>/talk/ghent/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/ghent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Listening effort during speech perception enhances auditory and lexical processing for non-native listeners and accents</title>
      <link>/publication/cognition/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0100</pubDate>
      
      <guid>/publication/cognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring cortical and subcortical responses to continuous speech</title>
      <link>/talk/aesop/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0100</pubDate>
      
      <guid>/talk/aesop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The role of semantic cues in prosodic disambiguation of wh-phrases in Korean</title>
      <link>/talk/baap/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0100</pubDate>
      
      <guid>/talk/baap/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The effects of adverse conditions on speech recognition by nonnative listeners: Electrophysiological and behavioural evidence</title>
      <link>/publication/phd/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/phd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Second language speech processing in realistic environments: An electrophysiological investigation</title>
      <link>/talk/snu/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0100</pubDate>
      
      <guid>/talk/snu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatic Sentence Stress Feedback for Non-native English Learners</title>
      <link>/publication/csl/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/csl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-linguistic perception of continuous speech: Neural entrainment to the speech amplitude envelope</title>
      <link>/talk/asa/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/talk/asa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Listening effort and multilingual speech communication: Neural measures of auditory and lexical processing by adults and older children</title>
      <link>/project/listening-effort/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0100</pubDate>
      
      <guid>/project/listening-effort/</guid>
      <description>&lt;p&gt;I am a postdoctoral reseacher on this ESRC grant (October 2017 - September 2020, PI: Prof. Paul Iverson). You can see a summary of this project here, and please find related work on Publications &amp;amp; Presentations.&lt;/p&gt;

&lt;p&gt;People feel like they need to &amp;ldquo;listen harder&amp;rdquo; when communicating in a second language, but it isn&amp;rsquo;t clear how this effort changes the brain processes involved in recognising speech. Our initial research has produced a surprising finding; we tested people who were trying to listen to a talker in a noisy background (i.e., a distracting talker), and found that auditory areas of the brain are better at picking out the target talker when people are listening to a second language than their first language. We did this by recording neural activity (electroencephalography; EEG) and measuring how it becomes entrained to the acoustics of speech. Although people would normally be expected to perform better when listening to their first language, we think that second-language listeners had more selective auditory processing because of their additional listening effort. We found related effects for neural measures of word recognition in the same task, and think that we&amp;rsquo;ve found mechanisms that allow second-language learners to partially compensate for their speech recognition difficulties. In this grant project, we will expand our investigation in a series of studies that manipulate the acoustics of speech, and compare how speech is recognised in first and second languages by speakers of English and Korean. Furthermore, we will test adults who learned both languages at the same time when they were young children, adults who learned their second language later in life, and older children who are in the process of learning both languages. Our goals are to understand how people can use listening effort to compensate for their difficulties with second-language speech, and examine how this ability develops and relates to proficiency. This work is important for understanding how people apply their processes and structures for language during everyday speech communication, and is relevant to a wide range of difficult listening conditions (e.g., hearing impairment). The work will also advance our scientific understanding of how new measures of neural entrainment for speech relate to practical aspects of speech recognition.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prosodic disambiguation of wh-words in Korean</title>
      <link>/project/korean-prosody/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0100</pubDate>
      
      <guid>/project/korean-prosody/</guid>
      <description>&lt;p&gt;I have been working on this project in collaboration with Prof. Jieun Kiaer at University of Oxford. In Korean, sentences with wh-words can be ambiguous between wh-questions and yes-no questions as a wh-word (e.g., 뭐) can be used either as a wh-pronoun (“what”) or an indefinite pronoun (“something”). Previous work has shown that prosody, especially accentual phrasing plays a role in diambiguating such sentences (Jun &amp;amp; Oh, 1996; Yun, 2012). However, yes-no questions often occur with certain experiential or attitudinal/softening adverbs (e.g., hanbun “once, sometime”, chom “please”). In this project, we are looking at how prosodic cues interact with these semantic cues when disambiguating wh sentences. For more information, please see my recent presentation at BAAP 2018.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
